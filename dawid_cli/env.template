# PLLuM Anonymizer - Environment Variables Template
# Copy this file to .env and fill in your values

# LLM Mode: "online" (PLLuM API) or "local" (Ollama)
# Can also be set in config.yaml
LLM_MODE=online

# Online Mode (PLLuM API) Configuration
# If not set, the system will use Faker for synthetic data generation
PLLUM_API_KEY=your_api_key_here
PLLUM_BASE_URL=https://apim-pllum-tst-pcn.azure-api.net/vllm/v1
PLLUM_MODEL_NAME=CYFRAGOVPL/pllum-12b-nc-chat-250715
PLLUM_TEMPERATURE=0.7
PLLUM_MAX_TOKENS=300

# Local Mode (Ollama) Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_NAME=PRIHLOP/PLLuM:latest
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=300

# Force GPU usage with Ollama (use direct API instead of ChatOllama)
# Set to "true" to use direct Ollama API (same as 'ollama run' - forces GPU)
# Set to "false" to use ChatOllama from langchain (Ollama auto-optimizes GPU, but may not force it)
# 
# According to LangChain docs: https://docs.langchain.com/oss/python/integrations/chat/ollama
# ChatOllama automatically optimizes GPU usage, but direct API uses the same endpoint as 'ollama run'
OLLAMA_USE_DIRECT_API=true

# Optional: Number of GPU layers for ChatOllama (if OLLAMA_USE_DIRECT_API=false)
# This is passed via model_kwargs to ChatOllama
# OLLAMA_NUM_GPU=1

# Use LLM for generation (true/false)
# If false, uses Faker only
USE_LLM_FOR_SYNTHESIS=false

